现在我们对生产者和消费者的工作机制有了基本的理解，让我们讨论一下Kafka在生产者和消费者间提供的语义保证。非常清楚的是，可以提供很多种可能的消息传递保证：

 - *At most once*-消息或许会丢失但不会重传。
 - *At least once*-消息永远不会丢失但或许被重传
 - *Exactly once*-这正是大家想要的，每条消息都会被传递一次，且只被传递一次。

值得注意的是，这分为两个问题：发布消息的持久化保证和消费消息的保证。

很多系统声称提供"eactly once"传递语义，但是阅读这些描述很重要，这些声明大多数都是误导性的（例如，他们没有考虑消费者和生产者可能失败的情况，以及存在多个消费者进行处理的情况，或者写入磁盘的数据可能丢失的情况）。

Kafka的语义更直截了当。当发布消息时，我们有一个概念，消息被"commiterd"到日志文件中。一旦已发布的消息处于"commited"状态，他就不会丢失，只要一台复制过该消息写入的分区的broker保持"alive"。下一节将更加详细的描述已提交的消息的定义，存活分区以及我们试图处理的故障类型的描述。现在，我们假想一个完美的，不会出故障的broker来理解消费者和生产者的担保机制。如果一个生产者试图发布一个消息并且发生了一次网络故障，无法确定这个故障时发生在消息发送之前还是消息提交之后。这和插入一条自增长主键的数据到数据库表中的主键的语义相似。

在0.11.0.0之前的版本中，如果生产者没有受到表明消息已提交的响应，那么生产者除了将消息重传之外别无选择。这里提供的是at-least-once的消息分发语义，因为如果最初的请求实际上成功了，那么在消息重传期期间，消息或许再次写入到日志文件中，。从0.11.0.0开始，Kafka生产者也新增了一种幂等性的分发策略，保证重传不会导致日志中有重复的实体。为了实现这个目的，broker给每个生产者分配一个ID，并且生产者给每条消息分配了序列号来避免产生重复的消息。也是从0.11.0.0开始，生产者新增使用类似事务的语义发送消息到多个topic的分区：例如，要么所有的消息都写入成功要么都失败。这种语义主要的应用场景是Kafka topic之间的exactly-once数据传送（如下所述）。

现在我们从消费者的角度描述下转发策略。所有的副本在相同的偏移量上有精确相同的日志。消费者控制在它在日志文件中的位置。如果消费者从不崩溃，它可以在内存中保存该位置，但是如果消费者崩溃了，我们想要这个主题分区被其他进程消费，新进程需要选择一个恰当的位置开始消费。假设消费者读取一些消息--它有几种选择来处理消息更新它的位置

1. 它可以先读取消息，然后在日志文件中保存位置，最后处理消息。这种情形下，消费者可能在保存它的位置之后，保存消息处理结果之前发生崩溃。这种场景中，代替原来执行中的进程的进程会从保存的位置开始执行，即使在改位置之前有少量的消息还没有处理。在消费者发生故障的情况下，这与"at-most-once"策略对应，消息可能没有被处理。  
2. 消费者可以读取消息，处理消息，最终保存它的位置。这种场景下，消费者可能在处理消息之后保存位置之前发生故障。这种场景下，当新进程处理收到的前几条消息时，这些消息已经被消费过了。在消费者发生故障的场景中，对应了"at-least-once"语义。在很多情形下，消息有一个主键因此更新是幂等的（接收到同一条消息两次，仅仅使用它本身的拷贝副本来覆盖一条记录）。


所以什么是exactly once语义（即你真正想要的）？当从一个Kafka主题消费消息同时向输出到另一个主题时（类似在Kafka Stream应用中），我们可以借助上述提到的0.11.0.0版本新事务型生产者。消费者的位置作为一条消息存储在主题中，所以我们可以在输出主题接收处理过的数据的时候，在同一事务中将偏移量写入Kafaka。如果事务被中断，该消费者的位置会被重置为原来的值并且在输出主题中产生的数据对其他消费者是否可见，依赖于他们的隔离级别。在默认的读未提交级别下，所有的消息对消费者可见，即使他们是终止事务的一部分，但是在读已提交下，消费者只能访问已提交事务中的消息（以及任何不属于事务的消息）。

在写入外部系统的应用场景中，限制在于需要在消费者的位置与实际存储为输出的内容间进行协调。解决这种问题的经典方法是在消费者位置的存储和消费者输出结果的存储之间引入两阶段提交。但这可以用更简单的方法处理，而且通常的做法是让消费者将其偏移量存储在与其输出相同的位置。这也是一种更好的方式，因为大部分消费者想写入的输出系统不支持双阶段提交。举个例子，Kafka Connect连接器，它将读取到的数据和数据的偏移量一起写入HDFS，因此保证了要不数据和数据偏移量一起更新要么都不更新。对于其他很多需要较强语义，并且没有主键来避免消息重复的数据系统来说，我们也遵循类似的模式。

因此，事实上Kafka在Kafka Stream中支持了exactly-once的消息传送功能，并且在Kafka主题间传输和处理数据时，通常使用事务型生成者/消费者提供exactly-once的消息传送功能。向其他目的系统e进行xactly-once的消息传送通常需要和这种系统协作，但Kafka提供了偏移量，使得这种应用场景得实现变得可行（参考Kafka Connect）。然而，Kafka默认保证消息的at-least-once传送，并且允许用户实现at-most-once的消息发送，通过禁止生产者重试，消费者在处理批量信息之前提交偏移量。