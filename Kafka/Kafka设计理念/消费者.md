Kafka消费者通过向负责它要消费消息的broker发出拉取请求来工作。消费者在每个请求中指定消息在日志中的偏移量，接收到从改位置开始的块数据。因此消费者拥有该位置的重要控制权，并且如果需要的化可以回退到该位置再次消费数据。
- Push vs. pull
Kafka采用一种传统的设计，数据从生产者推送到broker，消费者从broker拉取数据。一些以日志为中心的系统，采用一种不同的基于推送的方式由下游推送消息。这两种方式都既有优点又有缺点。然而，基于推送的系统很难处理多种消费者，因为broker控制数据传输的速率。对于消费者来说，目标通常是可以以尽可能大的速率消费数据，不幸的是，对于推送系统来说这意味着消费者的消费速率低于生产速率时消费者往往不堪重负（本质上是拒绝服务攻击）。一个基于拉取的系统拥有更好的性质：消费者的速率落后于生产者时，可以在适当的时间追赶上来。通过使用某种backoff协议，这个问题可以减轻，消费者可以通过这种协议表明它已经不堪重负了，但是想要充分利用（但不是过度利用）消费者的传输速率，就比看起来的要棘手的多。先前以这种方式构建系统的尝试使我们选择一种更传统的拉取模型。
基于拉取的系统的另外一个优点是它有助于对发送给消费者的数据做积极的批处理。基于推送的系统必须选择是立即发送一个请求还是堆积更多数据在晚些时候发送，而无需知道下游消费者何时可以立即处理信息。如果系统调整为低延迟状态，这就会导致一次只发送一条消息，以至于传输的数据不再被缓冲，这种方式是极度浪费的。一个基于拉取的设计解决了这个问题，因为消费者总是拉取在它的日志文件中当前位置之后的所有可用的消息（或者是配置的最大值）。所以消费者得到最佳的批消息而无需引入不必要的延时。
简单的基于拉取的系统的不足之处是如果broker没有数据，消费者或许会结束轮询，有效的[[busy-waiting]]数据到来。为了避免这种busy-waiting，Kafka在拉取请求提供了参数，允许消费者在一个long-poll阻塞等待，知道数据到来（还可以选择等待给定字节长度的数据来确保传输长度）。
你可以想象其他可能的设计，仅仅只有端到端拉取。生产者在本地写入数据到本地日志文件，broker从生产者本地拉取数据而消费者从broker拉取数据。通常提到的还有一种store-and-forward式生产者。这是一种有趣的设计，但是我们认为不适用于Kafka的使用场景，这种场景下有成千上万的生产者。我们运行大规模持久化数据系统的经验使我们感觉到，横跨多个应用，涉及到成千上万个硬盘的系统事实上不会使事情更可靠，并且操作这个系统将会是一个噩梦。在实践中，我们发现可以通过大规模运行的带有强大的SLAs的pipeline，无需生产者的持久化过程。
- Consumer Position
令人惊讶的是，持续追踪已经被消费的内容是消息系统的关键性能点之一。
大部分消息系统在broker上保存关于已经被消费的内容的元数据。那意味着，当一条消息被发送给消费者时，broker要么立即在本地记录该事件要么等待消费者的确认信号。这是一种相当直观的选择，事实上对于单机服务器，也没有其他地方可以存储这种状态信息。因为在很多消息系统中存储的数据结构规模很小，这也是一个很实用的选择-因为broker只要知道哪些消息被消费了，就可以在本地立即进行删除，保持较小的数据规模。
可能并不明显的是，使broker和消费者就被消费的数据保持一致性并不是一个小问题。如果broker在每条消息被发送到网络的时候，立即将标记为**consumed**，那么一旦消费者处理消息失败（可能由消费者崩溃或请求超时等原因），消息就会丢失。为了解决这个问题，很多消息系统添加了一个确认特性，意味着当消息被发送后仅仅被标记为**sent**而不是**consumed**，broker等待一个来自消费者发送的特定的确认，再将消息标记为**consumed**。这种策略解决了消息丢失的问题，但是引发了新问题。首先，如果消费者处理了消息但在发送确认之前失败了，消息将被消费两次。另一个问题是关于性能的，现在broker必须为每一条单独的消息保存多个状态（首先对其加锁，以防发送两次，然后将其永久地标记为**consumed**，以便可以删除该消息）。还有更棘手的问题要处理，例如如何处理已经被发送但一直得不到确认的消息。
Kafka用一种不同的方式处理这些问题。我们的topic被分成一组完全有序的分区，每个分区在任意给定时间内只能被每个订阅了这个topic的消费者组中的一个消费者消费。这就表明消费者在每个分区中的位置仅仅是一个单独的整数，下一条要消费的消息的偏移量。这使已经被消费的消息的状态信息相当少，每个分区仅仅是一个数字。这以非常低的代价实现了和消息确认机制等同的效果。
这个方案还有一个好处。一个消费者可以故意回退到一个已经消费过的偏移量重新处理数据。这违反了队列的通用的规约，但是对于多消费者来说是一个必要的特性。例如，如果消费者代码有一个bug，并且在消费了很多消息后才被发现，一旦bug被修复那些已经被消费的消息可以被重新消费。
- Offline Data Load
可伸缩的持久化特性允许消费者只进行周期性的消费，例如批量数据加载，周期性将数据加载到诸如Hadoop和关系型数据库之类的离线系统中。
在Hadoop的例子中，我们过将数据加载分配到多个独立的map任务来实现并行化，每个map任务负责一个node/topic/partition，从而打到充分并行化。Hadoop提供了任务管理机制，失败的任务可以重新启动而不会有重复数据的风险-只需要简单地从原来地位置重新启动即可。
- Static Membership
静态地成员关系目标是提升流应用的可用性，消费者组和其他应用构建于分组负载均衡协议之上。负载均衡协议基于分组协调分配组内成员实体id。这些生成的id是临时的，id在组内成员重新启动或者重新加入时会发生变化。对于基于消费者的app，在类似于代码部署，配置更新和周期性的重启的管理操作期间，这种动态成员关系会导致大量的任务重分配给不同的实例。对于大型的有状态的应用，在处理数据之前，清洗任务需要很长一段时间来恢复他们本地的状态，因此应用会部分或者完全不可用。被这种言论所激励，Kafka的分组协议允许组内成员提供持久化的id。基于这些id的组内成员关系保持不变，因此不会促发负载均衡。
如果你想使用静态的成员关系，
  - broker集群和客户端app都更新到2.3或以上版本，并且确保更新的broker使用的`inter.broker.protocol.version`是2.3或以上版本。
  - 为同一个组下的每个消费者配置项`ConsumerConfig#GROUP_INSTANCE_ID_CONFIG`配置唯一的值。
  - 对于Kafka流应用，为每个KafkaStreams实例的`ConsumerConfig#GROUP_INSTANCE_ID_CONFIG`配置唯一的值是充分的，和一个实例使用的线程个数无关。

如果你的broker版本小于2.3，但是你选择在客户端设置`ConsumerConfig#GROUP_INSTANCE_ID_CONFIG`参数，客户端会检测broker的版本，之后会抛出`UnsupportedException`异常。如果你不小心为不同实例配置了重复的id，broker侧的一种fencing机制会通知你重复的客户端通过触发`org.apache.kafka.common.errors.FencedInstanceIdException`异常来立即关闭。
